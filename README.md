<!--
  ___                     _      _               _              
 / _ \                   | |    | |             | |             
/ /_\ \__ _ _ __ __ _  __| | ___| |__   ___   __| | ___  ___    
|  _  / _` | '__/ _` |/ _` |/ _ \ '_ \ / _ \ / _` |/ _ \/ __|   
| | | | (_| | | | (_| | (_| |  __/ | | | (_) | (_| |  __/\__ \   
\_| |_/\__,_|_|  \__,_|\__,_|\___|_| |_|\___/ \__,_|\___||___/   

Welcome to **Ankelodon**, a modular multiâ€‘agent framework for complex question answering and data analysis.  
This project leverages [LangGraph](https://python.langgraph.org/) and [LangChain](https://python.langchain.com/) to orchestrate a suite of tools that can plan, execute and validate tasks on your behalf.

-->

# ğŸ§¬ Ankelodon Multiâ€‘Agent System

**Ankelodon** is a proofâ€‘ofâ€‘concept multiâ€‘tool agent inspired by the GAIA evaluation framework.  
It combines planning, execution and critique to solve openâ€‘ended queries that might involve search, file analysis, mathematics, coding or image understanding.  
By breaking down tasks into manageable steps and selecting the right tool for each job, Ankelodon aims to deliver accurate answers with verifiable evidence.

![project logo](docs/images/ankelodon_banner.png)

> *Note: The banner above is a placeholder. You can replace it with your own image placed at `docs/images/ankelodon_banner.png`.*

## ğŸŒŸ Features

### ğŸ§  Complexity assessment & routing

Before doing any heavy lifting, Ankelodon evaluates the incoming query to determine whether it requires planning or can be answered directly.  
Simple questions (e.g. definitions, single mathematical operations) are answered via a lightweight executor.  
Moderate and complex queries trigger the planner and agent pipeline, ensuring appropriate decomposition and tool usageã€942452390578334â€ L22-L34ã€‘.

### ğŸ§­ Structured planning

For nonâ€‘trivial tasks, a **planner** LLM generates a structured plan consisting of a series of steps.  
Each step has an ID, goal, selected tool, expected result and fallback strategy.  
The plan is stored as a Pydantic model (`PlannerPlan`) with strong typing for reliabilityã€981681905155103â€ L82-L100ã€‘.

### ğŸ¤– Agent execution

The **agent** node follows the plan stepâ€‘byâ€‘step.  
For each step it first produces reasoning, then invokes the suggested tool with the appropriate inputs.  
Tool outputs are captured and fed back into subsequent reasoning.  
The agent continues until all steps are complete or an error requires replanningã€981681905155103â€ L161-L186ã€‘.

### ğŸ§° Rich toolset

Ankelodon exposes a curated set of tools bound to the execution LLM:

| Tool | Purpose |
|---|---|
| `download_file_from_url` | Download files from the web by URL |
| `web_search` | Perform internet search via Tavily API |
| `arxiv_search` | Find relevant academic papers on arXiv |
| `wiki_search` | Fetch Wikipedia articles and summaries |
| `add`, `subtract`, `multiply`, `divide`, `power` | Basic arithmetic operations |
| `analyze_excel_file`, `analyze_csv_file` | Parse spreadsheets and compute statistics |
| `analyze_docx_file`, `analyze_pdf_file`, `analyze_txt_file` | Extract and summarise document content |
| `vision_qa_gemma` | Answer questions about images using a vision model |
| `safe_code_run` | Execute Python code securely in an isolated environment |

These tools are loaded into a `ToolNode` and passed to the agent for use during executionã€774776463100239â€ L10-L14ã€‘.

### ğŸ“ Comprehensive reporting & critique

After the agent finishes, a deterministic LLM generates a structured execution report.  
This report summarises the query, steps taken, key findings, sources used, and the final answer.  
A separate **critic** LLM evaluates the report for completeness, accuracy, methodology and evidence, scoring it out of 10 and suggesting improvements if necessaryã€981681905155103â€ L459-L525ã€‘.  
The system may then replan and reâ€‘execute until the answer meets quality thresholds.

## ğŸ— Architecture

Ankelodon is built as a directed acyclic graph of nodes. The highâ€‘level flow is:

1. **INPUT** â€“ Receive the user query and optional files.  
2. **COMPLEXITY_ASSESSOR** â€“ Classify the query as simple, moderate or complex and decide whether to plan.  
3. **PLANNING** â€“ Generate a multiâ€‘step plan when needed, using examples and strict rules about tool usage and numerical computation.  
4. **AGENT** â€“ Iterate through the plan: reason about each step, call a tool, capture results and update state.  
5. **TOOLS** â€“ Execute selected tools via a unified `ToolNode`.  
6. **FINALIZER** â€“ Consolidate the execution into a report and extract a formatted final answer.  
7. **CRITIC** â€“ Score the report and decide whether to accept or trigger the **REPLANNER**.  

The graph is compiled using LangGraphâ€™s `StateGraph` API and is flexible enough to be extended with new nodes or toolsã€942452390578334â€ L8-L50ã€‘.

## ğŸš€ Getting started

### Prerequisites

This project targets **Python 3.10+**. Youâ€™ll need API keys or credentials for any external services (e.g. OpenAI, Tavily, Gemini) used by tools.  
Assuming you have a virtual environment activated:

```bash
pip install langchain==0.1.* langgraph openai google-generativeai
# plus any other packages referenced in tools (pandas, numpy, pillow, tldextract, etc.)
```

### Running a simple query

The entry point is the `build_workflow` function in `src/agent.py`. It returns a compiled system you can invoke with a dictionary representing the agent state.  
A minimal example:

```python
from src.agent import build_workflow

# Initialize the graph
system = build_workflow()

# Build the initial state
state = {
    "query": "What is the square root of 144?",
    "messages": [],
    "files": [],
    "iteration_count": 0,
    "max_iterations": 3
}

# Invoke the system and get the result
result = system.invoke(state)
print(result.get("final_answer"))  # should output: FINAL ANSWER: 12
```

For more complex tasks involving file uploads or web searches, provide file paths in the `files` list and ensure appropriate API keys are set in the environment.

### Notebooks & examples

There are example notebooks under `src/` and `test_folder/` demonstrating how to test the agent with sample queries and data.  
Feel free to explore and adapt them to your own scenarios.

## ğŸ›£ Roadmap & GAIA adaptation

- Integrate unit conversion, date arithmetic and table operations to handle GAIA evaluation tasks outâ€‘ofâ€‘theâ€‘box.  
- Add questionâ€‘clarification and errorâ€‘recovery loops to minimise unnecessary replanning.  
- Streamline the tool list by removing unused tools and grouping related operations.  
- Improve caching of external calls (e.g. web search, downloads) to speed up repeated queries.  
- Expand the test suite and add continuous integration.

## ğŸ¤ Contributing

Contributions are welcome! If you find a bug or have an idea for improvement, feel free to open an issue or a pull request.  
When adding new tools or nodes, please ensure they adhere to the structured planning and execution patterns shown here, and update the tests accordingly.

## ğŸ“„ License

This project is released under the MIT License. See `LICENSE` for details.

---

*Ankelodon is a work in progress. Your feedback and useâ€‘cases will help shape its future. Happy hacking!* ğŸ¦¾
